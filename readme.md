## paper to read

* [paper_to_read](paper_to_read.md)



## Reference

* [A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/?fbclid=IwAR04HRMsK-KyEJ8XsQO7TABIf_yywmDhTEuZRxRMLhpYrn99XEwphYn7teI)
  *  [pdf](pdf/A_recipe_for_Training_NN.pdf) 
*  [(48) Henry AI Labs - YouTube](https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw/videos?fbclid=IwAR1GuEDrb7cOJ2-8XzziIoAqWkYG3jwxwACedIO-mk7Otszq2apP4oxPBv8) 
*  [Deep Learning Monitor - Find new Arxiv papers, tweets and Reddit posts for you](http://deeplearn.org/) 
*  [Browse the State-of-the-Art in Machine Learning](https://paperswithcode.com/sota) 
*  [Lunit Tech Blog](https://blog.lunit.io/) 
*  [Google AI Blog](https://ai.googleblog.com/) 
*   [Arxiv Sanity Preserver](http://www.arxiv-sanity.com/) 
*  [OpenAI Research](https://openai.com/research/#publications) 
*  [kaggle competitions](https://www.kaggle.com/competitions)

* [Khan Academy](https://www.khanacademy.org/) 
* [Bayesian Deep Learning](https://www.edwith.org/bayesiandeeplearning/joinLectures/14426)

## Top kagglers

* [limerobot](https://medium.com/kaggle-blog/zero-to-grandmaster-in-a-year-a-winners-interview-with-limerobot-18ddb3a1aae1)
* [Bestfitting](https://medium.com/kaggle-blog/profiling-top-kagglers-bestfitting-currently-1-in-the-world-58cc0e187b)
  * [pdf](./pdf/Bestfitting.pdf)

* [Kaggle winner's Blog](https://medium.com/kaggle-blog)

## Kaggle discussions

* [riid](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/210113)
* [data-science-bowl-2019](https://www.kaggle.com/c/data-science-bowl-2019/discussion/127891)



## Paper Review

* [XLNet: Generalized Autoregressive Pretraining for Language Understanding](./pdf/XLNet.pdf)
* [On the Variance of the Adaptive Learning Rate and Beyond](./pdf/RAdam)

*  [Evolved Transformer](pdf/Evolved_Transformer.pdf) 
*  [Cardiologist_Level_Arrhythmia_Detection_with_Convolutional_Neural_Networks](pdf/Cardiologist_Level_Arrhythmia_Detection_with_Convolutional_Neural_Networks.pdf) 

*  [Domain Adaptation](pdf/Domain_Adaptation.pdf) 
* [Language Models are Unsupervised Multitask Learners](./pdf/gpt2.pdf)
*  [Variants of Transformer](pdf/Variants_of_Transformer.pdf) 



## Google Scholar

* [my papers](https://scholar.google.com/citations?user=JxPwRDMAAAAJ&hl=en)

